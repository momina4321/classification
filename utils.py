# -*- coding: utf-8 -*-
"""FSL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16YER1JRRWKdnzl5n91lOPo8gY52m3Pjq
"""
import data as d
import numpy as np
import pandas as pd


def create_pairs(df):
    # we will create training and validation pairs
    pairs = np.empty((0, 2))
    pair_labels = []
    x = df["label"].unique()
    for lbl in range(len(x)):
        # forming same class pairs
        single_label = df[df["label"] == x[lbl]]  # df of samples belonging to the same class
        n = single_label.shape[0]  # total number of samples per class
        p = int((n * (n - 1)) / 2)  # number of pairs given n

        # p = min(p, max_size)
        # Choose 2p time series with random seed
        t1 = single_label.sample(n=p, replace=True).iloc[:, 1:3].values
        t2 = single_label.sample(n=p, replace=True).iloc[:, 1:3].values
        # print(t1)
        # Rescale time series to [0, 1] range
        ts1 = ((t1 - np.min(t1, axis=1).reshape(-1, 1)) / (np.max(t1, axis=1) - np.min(t1, axis=1)).reshape(-1, 1))
        ts2 = ((t2 - np.min(t2, axis=1).reshape(-1, 1)) / (np.max(t2, axis=1) - np.min(t2, axis=1)).reshape(-1, 1))
        print(t1, ts1)
        same_pairs = np.concatenate([ts1, ts2])
        # create different class pairs
        t1 = df[df["label"] == x[lbl]].sample(n=p, replace=True).iloc[:, 1:].values
        t2 = df[df["label"] != x[lbl]].sample(n=p, replace=True).iloc[:, 1:].values
        ts1 = ((t1 - np.min(t1, axis=1).reshape(-1, 1)) / (np.max(t1, axis=1) - np.min(t1, axis=1)).reshape(-1, 1))
        ts2 = ((t2 - np.min(t2, axis=1).reshape(-1, 1)) / (np.max(t2, axis=1) - np.min(t2, axis=1)).reshape(-1, 1))
        diff_pairs = np.concatenate([ts1, ts2])
        pairs = np.concatenate([same_pairs, diff_pairs])
        # Append the labels of the pairs
        pair_labels += ([1] * int(p) + [0] * int(p))

    return pairs, np.array(pair_labels)


# making pairs for siamese network that will be used to calculate similarity score

def create_supervised_task(train_data, val_data):
    # Create Ptr from Dtr
    train_pairs, train_labels = create_pairs(train_data)
    print(f'Done making training pairs')

    # Create Pval from Dval
    val_pairs, val_labels = create_pairs(val_data)
    print(f'Done making validation pairs')

    return train_pairs, train_labels, val_pairs, val_labels


def sample_k_shot_task(X, y, k, q, seed_state_support, seed_state_query):
    """
    :param X: input dataframe
    :param y: input labels
    :param k: k samples choosen per class
    :param q: number of queries per class
    :param seed_state_support: seed of support set
    :param seed_state_query: seed of query set
    :return:

    support_set: dataframe
    support_labels: array
    query_set: dataframe
    query_labels: array
    """

    support_set, query_set = pd.DataFrame(), pd.DataFrame()
    support_labels, query_labels = np.empty(0), np.empty(0)

    for label in np.sort(y.unique()):
        support_samples = X[y == label].sample(n=k, replace=False, random_state=seed_state_support)
        query_samples = X[y == label].sample(n=q, replace=False, random_state=seed_state_query)

        support_set = pd.concat([support_set, support_samples], axis=0)
        query_set = pd.concat([query_set, query_samples], axis=0)

        support_labels = np.concatenate((support_labels, np.array([label] * k)))
        query_labels = np.concatenate((query_labels, np.array([label] * q)))

    return support_set, support_labels, query_set, query_labels
